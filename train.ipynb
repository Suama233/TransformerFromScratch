{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分主要是由于 python 顺序执行 , 一些主要函数的辅助函数必须放到主要函数前面导致其出现时可能不知道要干什么 , 但先往下溯源到应用再回头看实现即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体上的流程 : \n",
    "\n",
    "- 配置 tokenizer , 模型 , dataset\n",
    "- 设置训练与评估流程\n",
    "- 运行 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`yield` 是干什么的 ? \n",
    "\n",
    "它主要是相当于一个函数级别的 iterator :\n",
    "- 被调用\n",
    "- 返回当下 iterator 值 \n",
    "- 步进 \n",
    "- 当下次被再调用时 , 不会从零开始 ( 这里对比 `return` )\n",
    "\n",
    "为什么要这样 : 这种流式的数据能优化性能 , 防止爆内存 . 而其应用的句子本身只是用来做一个统计与映射 , 没有过高的并行化计算要求 .\n",
    "\n",
    "如果用 return , 就需要一次性以 list ( 或类似东西 ) 将全体数据加载到内存里 :\n",
    "\n",
    "```python\n",
    "def get_all_sentences(ds, lang):\n",
    "    sentences = []\n",
    "    for item in ds:\n",
    "        sentences.append(item['translation'][lang])\n",
    "    return sentences\n",
    "```\n",
    "\n",
    "后面 hugging face 提供的 `train_from_iterator` 可以对接这种数据传输方式 , 具体如何实现不是这里的重点 .\n",
    "\n",
    "而可以发现只在这里用到了 这个 yield 与 iterator 机制 , 后面 train 时反而没有用 , 这是因为 Dataset 有自己的优化的读取方式 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对 tokenizer 的使用更多是照抄 huggingface 给的代码 , 因为整体步骤较为常见了 . \n",
    "\n",
    "整体代码的作用是构建一个从文本到 tokenizer ( 而非 tokens , 这一过程还需要加入特殊字符 ( 如 `[SOS] [PAD]` 等 , 在构建 Dataset 时才会去完成 ) ) \n",
    "\n",
    "这里的 tokenizer 在用语上也说是 \" 训练得到一个 tokenizer \" , 但与模型训练的训练不是一个概念 . tokenizer 的 \" 训练 \" 是一种统计学习 , 主要步骤有 : 遍历整个语料库 , 统计每个词出现的次数 , 过滤掉出现频率 < 2 的词 , 构建词典 等 . 其需要训练也主要是因为不同语料库的词频统计规律不同 ( 比如不同语言用的字典显然不同 , 一个魔幻小说用到的词用到一个论文里也会出现大量 `[UNK]` )\n",
    "\n",
    "这里的 tokenizer_path 能直接 format 是因为 config 文件中已经用了 `{0}` 占好位了 . \n",
    "\n",
    "一个 tokenizer 的配置 , 可以看到大概能找到三部分 : tokenizer 本身的原则 , pre_tokenizer 与 trainer\n",
    "\n",
    "Token 最基础的显然是单子划分原则 . 这里使用的 WordLevel 是最基础的一种 , 顾名思义 , 直接单个词记一个 token , 更复杂的划分方法当然还有很多 .\n",
    "\n",
    "pre_tokenizer 主要是对文本进行划分 : 这里的 Whitespace 相当于用空格进行划分 \n",
    "\n",
    "Trainer 则是指定需要插入的特殊字符 , 以及不被识别为 `[UNK]` 所需的最小频率等 .\n",
    "\n",
    "组合完成后就能训练并保存 , 以后就能直接调用了 .\n",
    "\n",
    "显然 tokenizer 是模块分离组合的产物 , 但为什么是这样的三部分分割设计 ( 比如为什么 pre_tokenizer 要单分出来之类的 ) , 我也不明白 , 还得多积累 ( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是数据集获取\n",
    "\n",
    "更准确来讲它还会承接将上面的 tokenzier 具体化的工作 ( tokenizer 不仅仅是参与词转 id , 还要在推理时参与 id 转词 )\n",
    "\n",
    "而切分步骤是在生成 Dataset 之前进行的 , 这个其实比较显然 . 而最关键的 Dataset 构造可以在另一个文件中看到 . \n",
    "\n",
    "max_len 是为了保证后续所有输入句子长度相同 , 需要到时候补 `[PAD]` . 而模型也会手动在 config 里设置超参数 seq_len , 根据这里的 max_len 选择 , 从而将一些过长的句子截断防止每个句子都有过长的 padding\n",
    "\n",
    "最终获得 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from dataset import BilingualDataset\n",
    "from model import build_transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取模型反而是最好理解的 , 毕竟其文件本身是自己写的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来这一部分是验证部分 . 其实可以最后再关注这部分而直接去看 train 函数的部分 , 最后再在函数里插入这里的验证模块而已 .\n",
    "\n",
    "首先对于单个待预测句子 , 选用贪心的策略 , 每次都选当下可能最高的那个 . \n",
    "\n",
    "处理上 , 首先将原句子 encode 好 , 扔到 encoder 里 , 再让目标句子从 SOS 开始生成 . 因为这里应用的是翻译任务 , 所以生成的本身也是一个完整句子 ( 而非某个从中间突然开始而缺少原句作为开头的句子 )\n",
    "\n",
    "这是 encoder-decoder 混合架构与单 decoder 架构的一个非常大的区别 . 如果是 decoder 任务 , 就会得到 `[[SOS] , en-sentence , [SEP] , it-sentence]` 这样的生成结构\n",
    "\n",
    "搞清楚到底哪里该输入什么 , 剩下的就简单了 : 将原句子放入 encoder , 让 decoder 一直生成 , 直到其生成 `[EOS]` 或最大长度到达原设定句子最大长度 . \n",
    "\n",
    "最后再压缩成一个向量尺寸的输出句子 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是预测函数 \n",
    "\n",
    "发现真的有用的部分就是 with torch.no_grad() 以后的部分 :\n",
    "\n",
    "每从验证集中取一个 batch , 取单个句子进行预测 , 预测结果与原结果进行展示 . 而显然翻译没法轻易得到 acc , 这里更多是展示翻译效果的作用 . \n",
    "\n",
    "里面会涉及到一些控制台调试之类的东西 , 与主线无关的就略去了 . writer 作为一个数据记录功能的加强 , if writer 部分代码也因与主线无关而略掉 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) \n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) \n",
    "\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from config import get_weights_file_path, latest_weights_file_path\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SummaryWriter 是一个记录版性质的东西 , 不影响核心 . 本质上删去这些记录功能 , 仅靠 torch 就能训练了 .\n",
    "\n",
    "整体上仍然是基本的模型训练结构 , 如果前面都能看懂的话这里就会非常容易 , 需要注意的更多是何时配置什么 , 使用什么之类的细节 .\n",
    "\n",
    "epoch 是指遍历一整个数据集 , step 是指经过的 batch 的数量 .\n",
    "\n",
    "padding 并非作为特殊字符参与训练 , 而是作为一个标志让模型的任何一部分都忽略掉他 , 比如 mask 的生成 , 比如交叉熵能设置忽略 padding . 而同行内对交叉熵用 label_smoothing 会将最大值的一部分概率分给别的 token , 来提升一定的表现 . \n",
    "\n",
    "注意 : 数据 , 模型 , loss 等是都要用 `.to(device)` 挪到 cuda 上的 .\n",
    "\n",
    "计算 loss 时 , 首先经过了展平操作 , 来将输入与 label 变成最简单的两个向量 . \n",
    "\n",
    "而验证与保存参数是每 epoch 为单位进行的 ( 而非每 step ) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) \n",
    "            decoder_input = batch['decoder_input'].to(device) \n",
    "            encoder_mask = batch['encoder_mask'].to(device) \n",
    "            decoder_mask = batch['decoder_mask'].to(device) \n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) \n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) \n",
    "            proj_output = model.project(decoder_output) \n",
    "\n",
    "            label = batch['label'].to(device) \n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 补充 \n",
    "\n",
    "### 总的 import 列表\n",
    "```python\n",
    "from model import build_transformer \n",
    "from dataset import BilingualDataset, causal_mask \n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split \n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "from datasets import load_dataset \n",
    "from tokenizers import Tokenizer \n",
    "from tokenizers.models import WordLevel \n",
    "from tokenizers.trainers import WordLevelTrainer \n",
    "from tokenizers.pre_tokenizers import Whitespace \n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformerFromScratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
